{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6672025277df>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ychen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Softmax Regression model \n",
    "# Nodes for input images and targe output classes \n",
    "# Used and modified by computation graph \n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "# Weight and bias\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# Initialize variables \n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Class and Loss Function \n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "# Cross entropy loss \n",
    "cross_entropy = tf.reduce_mean( \\\n",
    "        -tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))\n",
    "# reduce_sum: sum across classes\n",
    "# reduce_mean: average over sums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "# Steepest gradient descent, step length 0.5, cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# Adding operations to computation graph \n",
    "# Compute gradients, compute parameter update step, apply update step to parameters\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "# Load 100 training examples each iteration \n",
    "# Run train_step, using feed_dict to replace placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9191\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean( \\\n",
    "    tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Network \n",
    "# Weight initialization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution and Pooling \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, \\\n",
    "        W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], \\\n",
    "        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Convolutional Layer \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Convolutional Layer \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Densely Connected Layer \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-ec27083eb94b>:3: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Dropout \n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readout Layer \n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\n",
      "step 100, training accuracy 0.82\n",
      "step 200, training accuracy 0.88\n",
      "step 300, training accuracy 0.94\n",
      "step 400, training accuracy 0.9\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.92\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 1\n",
      "test accuracy 0.9606\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-layer model for MNIST classification \n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W_1 = weight_variable([784,100])\n",
    "b_1 = bias_variable([100])\n",
    "y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "W_2 = weight_variable([100, 10])\n",
    "b_2 = bias_variable([10])\n",
    "y = tf.matmul(y_1, W_2) + b_2\n",
    "\n",
    "# Initialize variables \n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Square loss\n",
    "square_loss = tf.losses.mean_squared_error(y, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# Steepest gradient descent, step length 0.5, cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(square_loss)\n",
    "# Adding operations to computation graph \n",
    "# Compute gradients, compute parameter update step, apply update step to parameters\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "# Evaluate the model \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean( \\\n",
    "    tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to Binary Labels\n",
    "def convertBinary(oneHotEncoding):\n",
    "    classes = np.where(oneHotEncoding == 1)[1]\n",
    "    binEncoding = np.reshape(np.unpackbits(classes.astype(np.uint8)),(classes.shape[0],8))[:,4:]\n",
    "    return binEncoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Layer with Binary Encoded Output\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "\n",
    "W_1 = weight_variable([784,100])\n",
    "b_1 = bias_variable([100])\n",
    "y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "W_2 = weight_variable([100, 4])\n",
    "b_2 = bias_variable([4])\n",
    "y = tf.matmul(y_1, W_2) + b_2\n",
    "\n",
    "# Initialize variables \n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Cross entropy loss \n",
    "square_loss = tf.losses.mean_squared_error(y, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6689\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# Steepest gradient descent, step length 0.5, cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(square_loss)\n",
    "# Adding operations to computation graph \n",
    "# Compute gradients, compute parameter update step, apply update step to parameters\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: convertBinary(batch[1])})\n",
    "\n",
    "# Evaluate the model \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean( \\\n",
    "    tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:convertBinary(mnist.test.labels)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-layer model for MNIST classification \n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W_1 = weight_variable([784,1])\n",
    "b_1 = bias_variable([1])\n",
    "y_1 = tf.matmul(x, W_1) + b_1\n",
    "\n",
    "W_2 = weight_variable([1, 10])\n",
    "b_2 = bias_variable([10])\n",
    "y = tf.nn.softmax(tf.matmul(y_1, W_2) + b_2)\n",
    "\n",
    "# Initialize variables \n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Square loss\n",
    "square_loss = tf.losses.mean_squared_error(y, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4008\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# Steepest gradient descent, step length 0.5, cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(square_loss)\n",
    "# Adding operations to computation graph \n",
    "# Compute gradients, compute parameter update step, apply update step to parameters\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "# Evaluate the model \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean( \\\n",
    "    tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
